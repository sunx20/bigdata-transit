{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunx20/bigdata-transit/blob/main/FinalProject_TransitTitan(New).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zxNqlqft9ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b57b1a7-16d6-4978-dc3b-06b7679e1de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting geopandas\n",
            "  Downloading geopandas-0.12.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyproj>=2.6.1.post1\n",
            "  Downloading pyproj-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas) (23.1)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.5.3)\n",
            "Requirement already satisfied: shapely>=1.7 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.0.1)\n",
            "Collecting fiona>=1.8\n",
            "  Downloading Fiona-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting munch>=2.3.2\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (2022.12.7)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (8.1.3)\n",
            "Collecting click-plugins>=1.0\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->geopandas) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->geopandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from munch>=2.3.2->fiona>=1.8->geopandas) (1.16.0)\n",
            "Installing collected packages: pyproj, munch, cligj, click-plugins, fiona, geopandas\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.3 geopandas-0.12.2 munch-2.5.0 pyproj-3.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.13.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install geopandas\n",
        "!pip install plotly\n",
        "\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import os\n",
        "from shapely.geometry import Point\n",
        "import json\n",
        "import plotly.graph_objects as go\n",
        "import datetime as dt\n",
        "import concurrent\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install pyarrow\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType, StringType, FloatType, ArrayType\n",
        "from pyspark.sql.functions import col, unix_timestamp, from_unixtime, to_utc_timestamp\n",
        "import boto3\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .config('spark.executor.cores', '16') \\\n",
        "    .config('spark.executor.memory', '80g') \\\n",
        "    .config('spark.driver.memory', '80g') \\\n",
        "    .config('spark.driver.maxResultSize', '0') \\\n",
        "    .config('spark.sql.session.timeZone', 'UTC') \\\n",
        "    .config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC') \\\n",
        "    .config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC') \\\n",
        "    .config('spark.sql.datetime.java8API.enabled', 'true') \\\n",
        "    .config('spark.sql.execution.arrow.pyspark.enabled', 'true') \\\n",
        "    .config('spark.sql.autoBroadcastJoinThreshold', '-1') \\\n",
        "    .config('spark.shuffle.spill', 'true') \\\n",
        "    .config('spark.driver.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true') \\\n",
        "    .config('spark.executor.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true') \\\n",
        "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
        "    .master('local[26]') \\\n",
        "    .appName('wego-daily') \\\n",
        "    .getOrCreate()\n",
        "spark.sparkContext.setLogLevel('ERROR')"
      ],
      "metadata": {
        "id": "8q9XFEWDuE3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce776c28-9ca9-41c8-9b4b-b2cb9aa27e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=a7cf514bad9c786afe2291dc6628c6b19ad486d7b4ecc76d2c6a75bf432c2c01\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace these with your own AWS access and secret keys\n",
        "aws_access_key_id = 'ASIAZPYTC6EZPXBA4WGK'\n",
        "aws_secret_access_key = 'UEh8Qw2aS0UeL+B+XX2Ry6EiTJRy2fmi9luU+jK1'\n",
        "\n",
        "# Set AWS credentials for Hadoop\n",
        "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", aws_access_key_id)\n",
        "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", aws_secret_access_key)\n",
        "\n",
        "# Read CSV from S3\n",
        "s3_path = \"s3://vandy-bigdata/transit-titans/weather/darksky_chattanooga.csv\"\n",
        "darksky = spark.read.csv(s3_path, header=True, inferSchema=True)\n",
        "\n",
        "# Print schema and show first 5 rows\n",
        "darksky.printSchema()\n",
        "darksky.show(5)\n",
        "\n",
        "# Convert 'time' column to 'datetime' and extract 'year', 'month', 'day', and 'hour'\n",
        "darksky = darksky.withColumn('datetime', from_unixtime(col('time') - 18000).cast('timestamp'))\n",
        "darksky = darksky.withColumn('year', col('datetime').cast('date').substr(1, 4).cast('int'))\n",
        "darksky = darksky.withColumn('month', col('datetime').cast('date').substr(6, 2).cast('int'))\n",
        "darksky = darksky.withColumn('day', col('datetime').cast('date').substr(9, 2).cast('int'))\n",
        "darksky = darksky.withColumn('hour', col('datetime').substr(12, 2).cast('int'))\n",
        "\n",
        "# Select columns, rename, and group by 'year', 'month', 'day', and 'hour'\n",
        "val_cols = ['temperature', 'nearest_storm_distance', 'precipitation_intensity', 'precipitation_probability']\n",
        "join_cols = ['year', 'month', 'day', 'hour']\n",
        "renamed_cols = {k: f\"darksky_{k}\" for k in val_cols}\n",
        "\n",
        "darksky = darksky.select(join_cols + val_cols)\n",
        "darksky = darksky.select([col(c).alias(renamed_cols.get(c, c)) for c in darksky.columns])\n",
        "darksky = darksky.groupBy(join_cols).mean().orderBy(join_cols)\n",
        "\n",
        "# Show first 20 rows\n",
        "darksky.show(20)"
      ],
      "metadata": {
        "id": "UMHfC7ApWyfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apcdata = spark.read.csv(\"s3://transit-titans/cartaapc_dashboard_2020.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# Add day and hour of day\n",
        "apcdata = apcdata.withColumn('year', F.year(apcdata.date))\n",
        "apcdata = apcdata.withColumn('month', F.month(apcdata.date))\n",
        "apcdata = apcdata.withColumn('day', F.dayofmonth(apcdata.date))\n",
        "apcdata = apcdata.withColumn('hour', F.hour(apcdata.actual_arrival_time))"
      ],
      "metadata": {
        "id": "p7SFWQnRabiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apcdata_joined = apcdata.join(darksky,on=['year', 'month', 'day', 'hour'], how='left')\n",
        "# Total number of rows and columns\n",
        "print(f\"Rows: {apcdata_joined.count():,}, Cols:{len(apcdata_joined.columns)}\")\n",
        "# Select a subset of columns to display\n",
        "selected_columns = ['year', 'month', 'day', 'hour', 'trip_id', 'stop_id', 'board_count', 'alight_count', 'occupancy', 'darksky_temperature']\n",
        "\n",
        "# Display the top 5 rows with only the selected columns\n",
        "apcdata_joined.select(selected_columns).show(5)\n",
        "apcdata_joined_pd = apcdata_joined.toPandas()"
      ],
      "metadata": {
        "id": "052P2_hCUpP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Register the DataFrame as a temporary view\n",
        "apcdata_joined.createOrReplaceTempView(\"apcdata_joined\")\n",
        "\n",
        "# Run the Spark SQL query\n",
        "result_df = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        year, \n",
        "        month, \n",
        "        day, \n",
        "        AVG(darksky_temperature) AS avg_temperature, \n",
        "        AVG(darksky_precipitation_intensity) AS avg_precipitation_intensity,\n",
        "        SUM(board_count) AS total_board_count\n",
        "    FROM apcdata_joined\n",
        "    GROUP BY year, month, day\n",
        "\"\"\")\n",
        "result_df = result_df.dropna()\n",
        "\n",
        "# Show the result DataFrame\n",
        "#result_df.show()\n",
        "result_pd = result_df.toPandas()\n",
        "\n",
        "corr = result_pd['avg_precipitation_intensity'].corr(result_pd['total_board_count'])\n",
        "sns.scatterplot(x='avg_precipitation_intensity', y='total_board_count', \n",
        "             data=result_pd, ci=None)\n",
        "sns.scatterplot(x='avg_temperature', y='total_board_count', \n",
        "             data=result_df, ci=None)"
      ],
      "metadata": {
        "id": "n8sDLzP0u6Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "PpdWUzR1u-Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqLqFAsp9mS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apcdata_joined_pd['geometry'] = apcdata_joined_pd.apply(lambda row: Point(row['stop_lon'], row['stop_lat']), axis=1)\n",
        "apcdata_joined_pd = gpd.GeoDataFrame(apcdata_joined_pd)\n",
        "apcdata_joined_pd = apcdata_joined_pd.set_geometry('geometry')\n",
        "apcdata_joined_pd['datetime'] = apcdata_joined_pd['actual_arrival_datetime'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d %H:%M:%S'))\n",
        "apcdata_joined_pd['datetime'] = pd.to_datetime(apcdata_joined_pd['datetime'])\n",
        "apcdata_joined_pd['date'] = pd.to_datetime(apcdata_joined_pd['trip_date'])\n",
        "print(f\"Date range: {apcdata_joined_pd['date'].min()} to {apcdata_joined_pd['date'].max()}\")\n",
        "print(len(apcdata_joined_pd))\n",
        "apcdata_joined_pd.head(1)"
      ],
      "metadata": {
        "id": "MNEJ5VXuvCFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "try:\n",
        "    with tempfile.TemporaryFile() as fp:\n",
        "        s3.Object(bucket_name, object_key).download_fileobj(fp)\n",
        "        fp.seek(0)\n",
        "        df_tracts = gpd.read_file(fp)\n",
        "except ClientError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "df_tracts['census_tract'] = df_tracts['GEOID'].apply(str)\n",
        "df_tracts.set_geometry('geometry', inplace=True)\n",
        "df_tracts = df_tracts[df_tracts['COUNTYFP']==\"065\"]\n",
        "print(len(df_tracts))\n",
        "df_tracts = df_tracts.drop_duplicates(subset=['census_tract'])\n",
        "print(len(df_tracts))\n",
        "df_tracts.head(1)"
      ],
      "metadata": {
        "id": "xTSU__livEfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the CRS class from pyproj\n",
        "from pyproj import CRS\n",
        "\n",
        "# Define the CRS for the GeoDataFrame df_tracts\n",
        "crs_tracts = CRS(\"EPSG:4269\")\n",
        "\n",
        "# Specify the CRS for the GeoDataFrame apcdata_joined_pd\n",
        "apcdata_joined_precovid.crs = crs_tracts\n",
        "apcdata_joined_postcovid.crs = crs_tracts\n",
        "# Perform a spatial join between apcdata_joined_pd and df_tracts based on the location of each bus stop\n",
        "df_merged_pre = gpd.sjoin(apcdata_joined_precovid, df_tracts, how=\"left\", predicate=\"within\")\n",
        "df_merged_post = gpd.sjoin(apcdata_joined_postcovid, df_tracts, how=\"left\", predicate=\"within\")\n",
        "# Check the length of df_merged to make sure no rows were dropped during the join\n",
        "print(len(df_merged_pre))\n",
        "print(len(df_merged_post))\n",
        "# Print the first few rows of the resulting merged GeoDataFrame\n",
        "df_merged_pre.head()"
      ],
      "metadata": {
        "id": "ftkl0QazvH3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "\n",
        "bucket_name = 'transit-titans'\n",
        "object_key = 'economic_data/census_economic_data_all_tennessee.csv'\n",
        "\n",
        "s3_client = boto3.client('s3',\n",
        "                         aws_access_key_id=aws_access_key_id,\n",
        "                         aws_secret_access_key=aws_secret_access_key)\n",
        "\n",
        "response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
        "file_content = response['Body'].read().decode('utf-8')\n",
        "df_econ = pd.read_csv(StringIO(file_content))\n",
        "\n",
        "def get_census_tract(row):\n",
        "    state, county, tract = str(row['state']), str(row['county']), str(row['tract'])\n",
        "    if len(county) == 2:\n",
        "        county = \"0{}\".format(county)\n",
        "    while len(county) != 3:\n",
        "        county = \"0\" + county\n",
        "    while len(tract) != 6:\n",
        "        tract = \"0\" + tract\n",
        "    return str(state + county + tract)\n",
        "\n",
        "df_econ['census_tract'] = df_econ.apply(lambda row: get_census_tract(row), axis=1)\n",
        "print(len(df_econ))\n",
        "df_econ = df_econ[df_econ['census_tract'].isin(df_tracts['census_tract'].tolist())]\n",
        "print(len(df_econ))\n",
        "df_econ.head(1)"
      ],
      "metadata": {
        "id": "ckl-pQWbDMu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_econ['total_population'] = df_econ['B01003_001E']\n",
        "df_econ['male_25_29'] = df_econ['B01001_011E']\n",
        "df_econ['female_25_29'] = df_econ['B01001_035E']\n",
        "df_econ['median_household_income'] = df_econ['B19013_001E']\n",
        "df_econ['median_family_income'] = df_econ['B19113_001E']\n",
        "df_econ['median_housing_value'] = df_econ['B25077_001E']\n",
        "df_econ['median_gross_rent'] = df_econ['B25064_001E']\n",
        "df_econ['vacant_housing_units'] = df_econ['B25002_003E']\n",
        "df_econ['total_housing_units'] = df_econ['B25001_001E']\n",
        "df_econ['white'] = df_econ['B02001_002E']\n",
        "df_econ['african_american'] = df_econ['B02001_003E']\n",
        "df_econ['hispanic'] = df_econ['B03001_003E']\n",
        "df_econ = df_econ[['total_population', 'male_25_29', 'female_25_29', 'median_household_income', 'median_family_income', 'median_housing_value', 'median_gross_rent', 'vacant_housing_units', 'total_housing_units', 'total_housing_units', 'white', 'african_american', 'hispanic', 'census_tract']]\n",
        "df_econ.head(1)"
      ],
      "metadata": {
        "id": "5kKoCBpfDO2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_tracts['census_tract'].unique()))\n",
        "print(len(df_econ['census_tract'].unique()))"
      ],
      "metadata": {
        "id": "XnWMZojSDXk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trip_list = spark.sql(\"SELECT DISTINCT trip_id FROM apcdata_joined\")\n",
        "trip_list.show()"
      ],
      "metadata": {
        "id": "ZoFB3YdVbSd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_pre = {'census_tract': [], 'board_count': [], 'board_count_per_day': [], 'alight_count_per_day': [], 'alight_count': [], 'geometry': []}\n",
        "print(len(df_tracts))\n",
        "\n",
        "num_days = len(df_merged_pre['date'].unique())\n",
        "counter = 0\n",
        "for k, v in df_tracts.iterrows():\n",
        "    df = df_merged_pre[df_merged_pre['geometry'].within(v['geometry'])] # geospatial query using geopandas\n",
        "    board_count = df['board_count'].sum()\n",
        "    board_count_per_day = board_count / num_days\n",
        "    alight_count = df['alight_count'].sum()\n",
        "    alight_count_per_day = alight_count / num_days\n",
        "    result_pre['census_tract'].append(v['census_tract'])\n",
        "    result_pre['board_count'].append(board_count)\n",
        "    result_pre['board_count_per_day'].append(board_count_per_day)\n",
        "    result_pre['alight_count_per_day'].append(alight_count_per_day)\n",
        "    result_pre['alight_count'].append(alight_count)\n",
        "    result_pre['geometry'].append(v['geometry'])\n",
        "    if counter % 10 == 0:\n",
        "        print(counter)\n",
        "    counter += 1"
      ],
      "metadata": {
        "id": "4A98wpWsDYDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_post = {'census_tract': [], 'board_count': [], 'board_count_per_day': [], 'alight_count_per_day': [], 'alight_count': [], 'geometry': []}\n",
        "print(len(df_tracts))\n",
        "\n",
        "num_days = len(df_merged_post['date'].unique())\n",
        "counter = 0\n",
        "for k, v in df_tracts.iterrows():\n",
        "    df = df_merged_post[df_merged_post['geometry'].within(v['geometry'])] # geospatial query using geopandas\n",
        "    board_count = df['board_count'].sum()\n",
        "    board_count_per_day = board_count / num_days\n",
        "    alight_count = df['alight_count'].sum()\n",
        "    alight_count_per_day = alight_count / num_days\n",
        "    result_post['census_tract'].append(v['census_tract'])\n",
        "    result_post['board_count'].append(board_count)\n",
        "    result_post['board_count_per_day'].append(board_count_per_day)\n",
        "    result_post['alight_count_per_day'].append(alight_count_per_day)\n",
        "    result_post['alight_count'].append(alight_count)\n",
        "    result_post['geometry'].append(v['geometry'])\n",
        "    if counter % 10 == 0:\n",
        "        print(counter)\n",
        "    counter += 1"
      ],
      "metadata": {
        "id": "TtZ9k1-vbtdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pickle\n",
        "import boto3\n",
        "\n",
        "df_result_pre = gpd.GeoDataFrame(result_pre)\n",
        "df_result_pre = df_result_pre.set_geometry('geometry')\n",
        "df_result_pre = df_result_pre.merge(right=df_econ, on='census_tract', validate='one_to_one')\n",
        "# Convert the GeoDataFrame to a Pickle object\n",
        "pickle_buffer = io.BytesIO()\n",
        "df_result_pre.to_pickle(pickle_buffer)\n",
        "\n",
        "# Create an S3 client\n",
        "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
        "\n",
        "# Set the S3 bucket and key (file path)\n",
        "bucket_name = 'transit-titans'\n",
        "output_key = 'output/df_processed.pkl'\n",
        "\n",
        "# Upload the Pickle object to S3\n",
        "s3.put_object(Bucket=bucket_name, Key=output_key, Body=pickle_buffer.getvalue(), ContentType='application/octet-stream')\n",
        "\n",
        "df_result_pre.head(20)"
      ],
      "metadata": {
        "id": "5JTlKyf8bui9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result_post = gpd.GeoDataFrame(result_post)\n",
        "df_result_post = df_result_post.set_geometry('geometry')\n",
        "df_result_post = df_result_post.merge(right=df_econ, on='census_tract', validate='one_to_one')\n",
        "\n",
        "# Convert the GeoDataFrame to a Pickle object\n",
        "pickle_buffer_post = io.BytesIO()\n",
        "df_result_post.to_pickle(pickle_buffer_post)\n",
        "\n",
        "# Set the S3 key (file path) for the post data\n",
        "output_key_post = 'output/df_processed_post.pkl'\n",
        "\n",
        "# Upload the Pickle object to S3\n",
        "s3.put_object(Bucket=bucket_name, Key=output_key_post, Body=pickle_buffer_post.getvalue(), ContentType='application/octet-stream')\n",
        "\n",
        "df_result_post.head(20)"
      ],
      "metadata": {
        "id": "rG-0d4rCDZfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result_pre_filtered = df_result_pre[['census_tract', 'board_count_per_day', 'geometry']]\n",
        "df_result_post_filtered = df_result_post[['census_tract', 'board_count_per_day']]"
      ],
      "metadata": {
        "id": "9nFZ025Wb7fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_econ1 = df_result_pre_filtered.merge(df_econ, on='census_tract', how='left')\n",
        "print(df_econ1.head(20))"
      ],
      "metadata": {
        "id": "JhnOmL7ib_hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import plotly.io as pio\n",
        "html_str = pio.to_html(fig, full_html=False)\n",
        "import boto3\n",
        "\n",
        "s3 = boto3.resource('s3',\n",
        "                    aws_access_key_id=aws_access_key_id,\n",
        "                    aws_secret_access_key=aws_secret_access_key)\n",
        "\n",
        "# Set the S3 bucket and key (file path)\n",
        "bucket_name = 'transit-titans'\n",
        "output_key = 'output/ChattanoogaECONChoropleth14.html'\n",
        "\n",
        "s3.Object(bucket_name, output_key).put(Body=html_str, ContentType='text/html')\n",
        "\n",
        "df_temp = df_econ1[['census_tract', 'median_household_income', 'geometry']]\n",
        "print(df_temp.head(20))\n",
        "\n",
        "# Convert the GeoDataFrame to a GeoJSON object\n",
        "geojson_temp = json.loads(df_temp.to_json())\n",
        "\n",
        "fig = go.Figure(go.Choroplethmapbox(geojson=geojson_temp,\n",
        "                                    featureidkey=\"properties.census_tract\",\n",
        "                                    locations=df_temp['census_tract'].tolist(),\n",
        "                                    z=df_temp['median_household_income'].tolist(), colorscale='Bluered', zmax=90000, zmin=10000,\n",
        "                                    marker_line_width=0.5, text=df_temp['median_household_income']))\n",
        "\n",
        "fig.update_layout(mapbox_style=\"light\", mapbox_accesstoken=mapbox_accesstoken,\n",
        "                  mapbox_zoom=11, mapbox_center={\"lat\": 35.03560, \"lon\": -85.25309}, title=\"Median Household Income\")"
      ],
      "metadata": {
        "id": "xRpWrcTfcE2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = df_result_pre_filtered.merge(df_result_post_filtered, on='census_tract', suffixes=('_pre', '_post'))\n",
        "df_merged['percentage_droped'] = (df_merged['board_count_per_day_post'] - df_merged['board_count_per_day_pre'])/df_merged['board_count_per_day_pre']\n",
        "df_merged = df_merged.dropna()\n",
        "df_merged.head(20)"
      ],
      "metadata": {
        "id": "p3c2Z20ydLzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the S3 bucket and key (file path)\n",
        "bucket_name = 'transit-titans'\n",
        "output_key = 'output/ChattanoogaCovidChangeChoropleth4.html'\n",
        "\n",
        "s3.Object(bucket_name, output_key).put(Body=html_str, ContentType='text/html')\n",
        "\n",
        "mapbox_accesstoken = \"pk.eyJ1IjoibXB3aWxidXIiLCJhIjoiY2thZmtvMXNzMDB3MzJ3bG1hZmtzZHNiMyJ9.wBKXW7tNj3G5cMk3k2tCEQ\"\n",
        "df_temp = df_merged\n",
        "\n",
        "# Calculate the percentage drop in board_count_per_day and add it as a new column in df_temp\n",
        "df_temp['percentage_droped'] = (df_temp['board_count_per_day_pre'] - df_temp['board_count_per_day_post']) / df_temp['board_count_per_day_pre']\n",
        "\n",
        "fig = go.Figure(go.Choroplethmapbox(geojson=json.loads(df_tracts.to_json()), \n",
        "                                    featureidkey=\"properties.census_tract\",\n",
        "                                    locations=df_temp['census_tract'].tolist(), \n",
        "                                    z=df_temp['percentage_droped'].tolist(), colorscale='Bluered', zmax=0, zmin=-0.6, text=\"properties.percentage_droped\"))\n",
        "\n",
        "fig.update_layout(mapbox_style=\"light\", mapbox_accesstoken=mapbox_accesstoken,\n",
        "                  mapbox_zoom=11, mapbox_center = {\"lat\": 35.03560, \"lon\": -85.25309}, title=\"Change\")"
      ],
      "metadata": {
        "id": "83PgSax2dcDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "End of mine"
      ],
      "metadata": {
        "id": "Ng7XLuRseE4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged_c = df_merged\n",
        "most_used_routes = df_merged_c.groupby(['route_id'])['board_count'].sum().orderBy('board_count', ascending=False)\n",
        "most_used_routes.show(10)"
      ],
      "metadata": {
        "id": "sn7OuF0QkTxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the route IDs to filter\n",
        "route_ids = ['4', '1', '9', '16', '21']\n",
        "\n",
        "# Filter the DataFrame for the specified route IDs\n",
        "df_route = df_merged_c.filter(col('route_id').isin(route_ids))\n",
        "\n",
        "# Group the DataFrame by month and direction, and count the number of times each direction occurred in each month\n",
        "direction_counts = df_route.groupby(['year', 'month', 'direction_desc']).count().withColumnRenamed('count', 'route_id')\n",
        "\n",
        "# Pivot the resulting DataFrame to have the months as the rows, the directions as the columns, and the counts as the values\n",
        "direction_counts_pivot = direction_counts.groupBy(['year', 'month']).pivot('direction_desc').sum('route_id')"
      ],
      "metadata": {
        "id": "Ozpq2zqiBPGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "direction_counts_pivot"
      ],
      "metadata": {
        "id": "jJY01B6ZlMVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Define the route IDs to filter\n",
        "route_ids = ['4', '1', '9', '16', '21']\n",
        "\n",
        "# Filter the DataFrame for the specified route IDs\n",
        "df_route = df_merged_c.filter(df_merged_c['route_id'].isin(route_ids))\n",
        "\n",
        "# Group the DataFrame by month and direction, and count the number of times each direction occurred in each month\n",
        "direction_counts = df_route.groupby(['year', 'month', 'direction_desc']).agg({'route_id': 'count'}).withColumnRenamed('count(route_id)', 'route_id')\n",
        "\n",
        "# Convert the Spark DataFrame to Pandas for plotting with Plotly\n",
        "direction_counts_pandas = direction_counts.toPandas()\n",
        "\n",
        "# Create a bar chart showing inbound and outbound of each month\n",
        "fig = px.bar(direction_counts_pandas, x='month', y='route_id', color='direction_desc', barmode='group', title='Route Direction Counts by Month')\n",
        "fig.update_xaxes(title='Month')\n",
        "fig.update_yaxes(title='Direction Count')\n",
        "\n",
        "# Display the chart\n",
        "fig.write_html('/Users/qiujuan/downloads/transit/new.html')"
      ],
      "metadata": {
        "id": "_C22CBedlNDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max, to_timestamp\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Filter the DataFrame for only route 4\n",
        "df_route4 = df_apc.filter(df_apc.route_id.isin(route_ids)).select(\"*\")\n",
        "\n",
        "# Convert the relevant columns to datetime format\n",
        "df_route4 = df_route4.withColumn(\"trip_start_time\", to_timestamp(df_route4[\"trip_start_time\"], \"HH:mm:ss\"))\n",
        "df_route4 = df_route4.withColumn(\"actual_arrival_time\", to_timestamp(df_route4[\"actual_arrival_time\"], \"HH:mm:ss\"))\n",
        "\n",
        "# Group the DataFrame by trip_id and get the latest actual_arrival_datetime for each trip\n",
        "latest_arrival_times = df_route4.groupBy(\"date\", \"trip_id\").agg(max(\"actual_arrival_time\").alias(\"actual_arrival_time_latest\"))\n",
        "\n",
        "# Merge the latest arrival times back into the DataFrame\n",
        "df_route4_with_latest_arrivals = df_route4.join(latest_arrival_times, on=[\"date\", \"trip_id\"])\n",
        "\n",
        "# Calculate the trip time as the difference between trip_start and the latest actual_arrival_datetime\n",
        "df_route4_with_latest_arrivals = df_route4_with_latest_arrivals.withColumn(\"trip_time\", (df_route4_with_latest_arrivals[\"actual_arrival_time_latest\"].cast(\"long\") - df_route4_with_latest_arrivals[\"trip_start_time\"].cast(\"long\")).cast(IntegerType()))\n",
        "\n",
        "df_route4_with_latest_arrivals_unique = df_route4_with_latest_arrivals.dropDuplicates([\"date\", \"trip_id\"])\n",
        "\n",
        "trip_time_sum = df_route4_with_latest_arrivals_unique.groupBy(\"month\").agg({\"trip_time\": \"sum\"}).withColumnRenamed(\"sum(trip_time)\", \"trip_time_sum\")"
      ],
      "metadata": {
        "id": "e6mYk1w2ltFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trip_time_sum"
      ],
      "metadata": {
        "id": "Z4g-s95vl0To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_timestamp\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Filter the DataFrame for only route 4\n",
        "df_route4 = df_apc.filter(col('route_id').isin(route_ids))\n",
        "\n",
        "# Convert the relevant columns to timestamp format\n",
        "df_route4 = df_route4.withColumn('trip_start_time', to_timestamp('trip_start_time'))\n",
        "df_route4 = df_route4.withColumn('actual_arrival_time', to_timestamp('actual_arrival_time'))\n",
        "\n",
        "# Group the DataFrame by trip_id and get the latest actual_arrival_datetime for each trip\n",
        "latest_arrival_times = df_route4.groupby('date', 'trip_id').agg({'actual_arrival_time': 'max'})\n",
        "\n",
        "# Merge the latest arrival times back into the DataFrame\n",
        "df_route4_with_latest_arrivals = df_route4.join(latest_arrival_times, on=['date', 'trip_id'])\n",
        "df_route4_with_latest_arrivals = df_route4_with_latest_arrivals.withColumnRenamed('max(actual_arrival_time)', 'actual_arrival_time_latest')\n",
        "\n",
        "# Calculate the trip time as the difference between trip_start and the latest actual_arrival_datetime\n",
        "df_route4_with_latest_arrivals = df_route4_with_latest_arrivals.withColumn('trip_time', col('actual_arrival_time_latest') - col('trip_start_time'))\n",
        "\n",
        "df_route4_with_latest_arrivals_unique = df_route4_with_latest_arrivals.dropDuplicates(['date', 'trip_id'])\n",
        "\n",
        "# Group the DataFrame by month and sum the trip time\n",
        "trip_time_sum = df_route4_with_latest_arrivals_unique.groupby('month').agg({'trip_time': 'sum'}).withColumnRenamed('sum(trip_time)', 'total_trip_time')\n",
        "\n",
        "# Convert the trip time to days\n",
        "trip_time_sum = trip_time_sum.withColumn('total_trip_time', col('total_trip_time') / (24*3600))\n",
        "\n",
        "# Convert the DataFrame to Pandas for plotting\n",
        "trip_time_sum_pd = trip_time_sum.toPandas()\n",
        "\n",
        "# Create the plot\n",
        "fig = go.Figure(go.Bar(x=trip_time_sum_pd['month'], y=trip_time_sum_pd['total_trip_time']))\n",
        "fig.update_layout(\n",
        "    title='Total Trip Time for Top Used 5 Routes',\n",
        "    xaxis_title='Month',\n",
        "    yaxis_title='Total Trip Time (days)',\n",
        "    margin=dict(l=50, r=50, t=50, b=50),\n",
        "    font=dict(size=12),\n",
        ")\n",
        "\n",
        "# Save the plot as an HTML file\n",
        "fig.write_html('/Users/qiujuan/downloads/transit/xx.html')\n"
      ],
      "metadata": {
        "id": "ZiofOBytmAxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tb3vGikxBXAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G149rMygBaNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "djYXER9COxqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}